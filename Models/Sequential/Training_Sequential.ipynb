{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4HjW4CHbRNq",
        "outputId": "e89ddcc8-faf8-4c36-ab67-1e172783e8a1"
      },
      "outputs": [],
      "source": [
        "#!pip install matplotlib\n",
        "#!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "urVnsyjGbbII"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-0qb4MvtJN"
      },
      "source": [
        "#  Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDk8VGP32SDx",
        "outputId": "06fa4d5f-104d-4b02-bb77-6abe7154e257"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Directory to check for existence\n",
        "train_dir = '../train'\n",
        "# Directory to check for existence\n",
        "valid_dir = '../valid'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ofuu8rlg2eIa"
      },
      "outputs": [],
      "source": [
        "height,width=180,180\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Augment the data\n",
        "#data_augmentation = tf.keras.Sequential([\n",
        "#    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "#    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "#    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "#    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n",
        "#    tf.keras.layers.experimental.preprocessing.RandomTranslation(\n",
        "#        height_factor=0.2, \n",
        "#        width_factor=0.2, \n",
        "#        fill_mode='reflect'\n",
        "#    ),\n",
        "#])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXnhnIlC2gT5",
        "outputId": "43a41223-6b38-4147-a897-637d72c0dd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 81946 files belonging to 10 classes.\n",
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "#train_set = tf.keras.preprocessing.image_dataset_from_directory(train_dir,image_size=(height,width))\n",
        "#valid_set = tf.keras.preprocessing.image_dataset_from_directory(valid_dir,image_size=(height,width))\n",
        "\n",
        "\n",
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(height, width),\n",
        "    shuffle=True,\n",
        ")\n",
        "class_names = train_set.class_names\n",
        "#train_set = train_set.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    valid_dir,\n",
        "    image_size=(height, width),\n",
        ")\n",
        "\n",
        "#batches = 20\n",
        "#train_set = train_set.take(batches)\n",
        "#valid_set = valid_set.take(batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m5QE94f2jCB",
        "outputId": "18aaadcc-fd59-4e71-f858-0226282052cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(valid_dir,image_size=(height,width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Flatten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 9590.3555 - accuracy: 0.1156 - val_loss: 1518.0831 - val_accuracy: 0.2259\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 443.2111 - accuracy: 0.3703 - val_loss: 57.7588 - val_accuracy: 0.3708\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 27s 1s/step - loss: 13.9070 - accuracy: 0.7250 - val_loss: 37.3726 - val_accuracy: 0.3914\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 5.3668 - accuracy: 0.8531 - val_loss: 25.9697 - val_accuracy: 0.4610\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 1.9982 - accuracy: 0.9281 - val_loss: 25.1817 - val_accuracy: 0.4737\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 24s 1s/step - loss: 1.0510 - accuracy: 0.9609 - val_loss: 25.7674 - val_accuracy: 0.4844\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 24s 1s/step - loss: 1.6833 - accuracy: 0.9594 - val_loss: 26.5197 - val_accuracy: 0.4705\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.7935 - accuracy: 0.9688 - val_loss: 24.7660 - val_accuracy: 0.4730\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.4292 - accuracy: 0.9719 - val_loss: 24.5461 - val_accuracy: 0.4799\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.6088 - accuracy: 0.9828 - val_loss: 24.1124 - val_accuracy: 0.4813\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# Define your model architecture\n",
        "model = Sequential([\n",
        "    # Add layers according to your requirement\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # Adjust the number of units to match the number of classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_set, validation_data=valid_set, epochs=10)\n",
        "model.save_weights('sequential.h5')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
