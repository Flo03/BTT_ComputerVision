{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4HjW4CHbRNq",
        "outputId": "e89ddcc8-faf8-4c36-ab67-1e172783e8a1"
      },
      "outputs": [],
      "source": [
        "#!pip install matplotlib\n",
        "#!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "urVnsyjGbbII"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-0qb4MvtJN"
      },
      "source": [
        "#  Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDk8VGP32SDx",
        "outputId": "06fa4d5f-104d-4b02-bb77-6abe7154e257"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Directory to check for existence\n",
        "train_dir = '../../train'\n",
        "# Directory to check for existence\n",
        "valid_dir = '../../valid'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ofuu8rlg2eIa"
      },
      "outputs": [],
      "source": [
        "height,width=180,180\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_height = 180\n",
        "final_width = 180\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomTranslation(\n",
        "        height_factor=0.1, \n",
        "        width_factor=0.1, \n",
        "        fill_mode='reflect'\n",
        "    ),\n",
        "    # Resizing layer to ensure all images are of the same size\n",
        "    tf.keras.layers.experimental.preprocessing.Resizing(final_height, final_width),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXnhnIlC2gT5",
        "outputId": "43a41223-6b38-4147-a897-637d72c0dd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 81946 files belonging to 10 classes.\n",
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(height, width),\n",
        "    shuffle=True,\n",
        ")\n",
        "class_names = train_set.class_names\n",
        "#train_set = train_set.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    valid_dir,\n",
        "    image_size=(height, width),\n",
        ")\n",
        "\n",
        "\n",
        "#batches = 20\n",
        "#train_set = train_set.take(batches)\n",
        "#valid_set = valid_set.take(batches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 81946 files belonging to 10 classes.\n",
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(height, width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    valid_dir,\n",
        "    image_size=(height, width),\n",
        "    batch_size=batch_size  # Keep validation batch size the same for consistency\n",
        ")\n",
        "\n",
        "train_set = train_set.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "train_set = train_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "valid_set = valid_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m5QE94f2jCB",
        "outputId": "18aaadcc-fd59-4e71-f858-0226282052cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(valid_dir,image_size=(height,width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "# Load the EfficientNetV2S model without the top layer\n",
        "base_model = tf.keras.applications.EfficientNetV2S(\n",
        "    include_top=False,  # Do not include the fully connected layer at the top\n",
        "    weights=\"imagenet\",  # Use weights pre-trained on ImageNet\n",
        "    input_tensor=None,  # Optional custom input tensor\n",
        "    input_shape=None,  # Specify your custom input shape if necessary\n",
        "    pooling=None,  # Specify this if you need a specific type of pooling\n",
        "    include_preprocessing=True  # Include preprocessing according to the model's requirements\n",
        ")\n",
        "\n",
        "# Make sure the base model is not trainable\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Add a logistic layer for 10 classes\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# This is the model we will train\n",
        "efficient_net_new = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "efficient_net_new.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image, label):\n",
        "    resized_image = tf.image.resize(image, [384, 384])  # Resize to the expected input size\n",
        "    return resized_image, label\n",
        "\n",
        "# Assuming `train_dataset` and `valid_dataset` are your original datasets\n",
        "train_dataset_resized = train_set.map(preprocess_image)\n",
        "valid_dataset_resized = valid_set.map(preprocess_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2561/2561 [==============================] - 1371s 535ms/step - loss: 0.3168 - accuracy: 0.8887 - val_loss: 0.2799 - val_accuracy: 0.9000\n",
            "Epoch 2/30\n",
            "2561/2561 [==============================] - 1434s 560ms/step - loss: 0.2480 - accuracy: 0.9121 - val_loss: 0.2769 - val_accuracy: 0.9050\n",
            "Epoch 3/30\n",
            "2561/2561 [==============================] - 1383s 540ms/step - loss: 0.2237 - accuracy: 0.9205 - val_loss: 0.2573 - val_accuracy: 0.9104\n",
            "Epoch 4/30\n",
            "2561/2561 [==============================] - 1410s 550ms/step - loss: 0.2092 - accuracy: 0.9254 - val_loss: 0.2071 - val_accuracy: 0.9291\n",
            "Epoch 5/30\n",
            "2561/2561 [==============================] - 1429s 558ms/step - loss: 0.2013 - accuracy: 0.9289 - val_loss: 0.2046 - val_accuracy: 0.9302\n",
            "Epoch 6/30\n",
            "2561/2561 [==============================] - 1438s 561ms/step - loss: 0.1955 - accuracy: 0.9306 - val_loss: 0.2201 - val_accuracy: 0.9290\n",
            "Epoch 7/30\n",
            "2561/2561 [==============================] - 1515s 592ms/step - loss: 0.1912 - accuracy: 0.9325 - val_loss: 0.1971 - val_accuracy: 0.9355\n",
            "Epoch 8/30\n",
            "2561/2561 [==============================] - 1585s 619ms/step - loss: 0.1883 - accuracy: 0.9345 - val_loss: 0.1999 - val_accuracy: 0.9394\n",
            "Epoch 9/30\n",
            "2561/2561 [==============================] - 1561s 609ms/step - loss: 0.1799 - accuracy: 0.9364 - val_loss: 0.2049 - val_accuracy: 0.9362\n",
            "Epoch 10/30\n",
            "2561/2561 [==============================] - 1526s 596ms/step - loss: 0.1795 - accuracy: 0.9374 - val_loss: 0.1963 - val_accuracy: 0.9407\n",
            "Epoch 11/30\n",
            "2561/2561 [==============================] - 1490s 582ms/step - loss: 0.1747 - accuracy: 0.9392 - val_loss: 0.1960 - val_accuracy: 0.9415\n",
            "Epoch 12/30\n",
            "2561/2561 [==============================] - 1448s 565ms/step - loss: 0.1718 - accuracy: 0.9395 - val_loss: 0.2005 - val_accuracy: 0.9385\n",
            "Epoch 13/30\n",
            "2561/2561 [==============================] - 4288s 2s/step - loss: 0.1698 - accuracy: 0.9405 - val_loss: 0.2010 - val_accuracy: 0.9367\n",
            "Epoch 14/30\n",
            "2561/2561 [==============================] - 1514s 591ms/step - loss: 0.1672 - accuracy: 0.9410 - val_loss: 0.2065 - val_accuracy: 0.9393\n",
            "Epoch 15/30\n",
            "2561/2561 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9414Restoring model weights from the end of the best epoch: 11.\n",
            "2561/2561 [==============================] - 1527s 596ms/step - loss: 0.1644 - accuracy: 0.9414 - val_loss: 0.2096 - val_accuracy: 0.9382\n",
            "Epoch 15: early stopping\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    min_delta=0.001,  # Minimum change to qualify as an improvement\n",
        "    patience=4,  # Stop after 10 epochs without improvement\n",
        "    verbose=1,  # Print a message when stopping\n",
        "    mode='min',  # Minimize the monitored quantity (val_loss)\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Train the model with the early stopping callback\n",
        "history = efficient_net_new.fit(\n",
        "    train_set,\n",
        "    epochs=30,  # Example number of epochs\n",
        "    validation_data=valid_set,\n",
        "    callbacks=[early_stopping]  # Add early stopping to the list of callbacks\n",
        ")\n",
        "\n",
        "efficient_net_new.save_weights('efficientnet_augmentation_full_dataset_epochs10.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
