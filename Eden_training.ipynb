{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4HjW4CHbRNq",
        "outputId": "e89ddcc8-faf8-4c36-ab67-1e172783e8a1"
      },
      "outputs": [],
      "source": [
        "#!pip install matplotlib\n",
        "#!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "urVnsyjGbbII"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-0qb4MvtJN"
      },
      "source": [
        "#  Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDk8VGP32SDx",
        "outputId": "06fa4d5f-104d-4b02-bb77-6abe7154e257"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Directory to check for existence\n",
        "train_dir = 'train'\n",
        "# Directory to check for existence\n",
        "valid_dir = 'valid'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ofuu8rlg2eIa"
      },
      "outputs": [],
      "source": [
        "height,width=180,180\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Augment the data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomTranslation(\n",
        "        height_factor=0.2, \n",
        "        width_factor=0.2, \n",
        "        fill_mode='reflect'\n",
        "    ),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXnhnIlC2gT5",
        "outputId": "43a41223-6b38-4147-a897-637d72c0dd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 81946 files belonging to 10 classes.\n",
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "#train_set = tf.keras.preprocessing.image_dataset_from_directory(train_dir,image_size=(height,width))\n",
        "#valid_set = tf.keras.preprocessing.image_dataset_from_directory(valid_dir,image_size=(height,width))\n",
        "\n",
        "\n",
        "\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(height, width),\n",
        "    shuffle=True,\n",
        ")\n",
        "class_names = train_set.class_names\n",
        "#train_set = train_set.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    valid_dir,\n",
        "    image_size=(height, width),\n",
        ")\n",
        "\n",
        "batches = 20\n",
        "train_set = train_set.take(batches)\n",
        "valid_set = valid_set.take(batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'_TakeDataset' object has no attribute 'class_names'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#train_set.class_names, valid_set.class_names # ensure the label in here are equal\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m class_names, \u001b[43mvalid_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_names\u001b[49m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_TakeDataset' object has no attribute 'class_names'"
          ]
        }
      ],
      "source": [
        "#train_set.class_names, valid_set.class_names # ensure the label in here are equal\n",
        "class_names, valid_set.class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m5QE94f2jCB",
        "outputId": "18aaadcc-fd59-4e71-f858-0226282052cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10244 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "valid_set = tf.keras.preprocessing.image_dataset_from_directory(valid_dir,image_size=(height,width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Flatten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 111/2561 [>.............................] - ETA: 7:47 - loss: 1163.1349 - accuracy: 0.1990"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# Define your model architecture\n",
        "model = Sequential([\n",
        "    # Add layers according to your requirement\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # Adjust the number of units to match the number of classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_set, validation_data=valid_set, epochs=10)\n",
        "model.save_weights('sequential.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer input_3: Trainable = False\n",
            "Layer conv1_pad: Trainable = False\n",
            "Layer conv1_conv: Trainable = False\n",
            "Layer conv1_bn: Trainable = False\n",
            "Layer conv1_relu: Trainable = False\n",
            "Layer pool1_pad: Trainable = False\n",
            "Layer pool1_pool: Trainable = False\n",
            "Layer conv2_block1_1_conv: Trainable = False\n",
            "Layer conv2_block1_1_bn: Trainable = False\n",
            "Layer conv2_block1_1_relu: Trainable = False\n",
            "Layer conv2_block1_2_conv: Trainable = False\n",
            "Layer conv2_block1_2_bn: Trainable = False\n",
            "Layer conv2_block1_2_relu: Trainable = False\n",
            "Layer conv2_block1_0_conv: Trainable = False\n",
            "Layer conv2_block1_3_conv: Trainable = False\n",
            "Layer conv2_block1_0_bn: Trainable = False\n",
            "Layer conv2_block1_3_bn: Trainable = False\n",
            "Layer conv2_block1_add: Trainable = False\n",
            "Layer conv2_block1_out: Trainable = False\n",
            "Layer conv2_block2_1_conv: Trainable = False\n",
            "Layer conv2_block2_1_bn: Trainable = False\n",
            "Layer conv2_block2_1_relu: Trainable = False\n",
            "Layer conv2_block2_2_conv: Trainable = False\n",
            "Layer conv2_block2_2_bn: Trainable = False\n",
            "Layer conv2_block2_2_relu: Trainable = False\n",
            "Layer conv2_block2_3_conv: Trainable = False\n",
            "Layer conv2_block2_3_bn: Trainable = False\n",
            "Layer conv2_block2_add: Trainable = False\n",
            "Layer conv2_block2_out: Trainable = False\n",
            "Layer conv2_block3_1_conv: Trainable = False\n",
            "Layer conv2_block3_1_bn: Trainable = False\n",
            "Layer conv2_block3_1_relu: Trainable = False\n",
            "Layer conv2_block3_2_conv: Trainable = False\n",
            "Layer conv2_block3_2_bn: Trainable = False\n",
            "Layer conv2_block3_2_relu: Trainable = False\n",
            "Layer conv2_block3_3_conv: Trainable = False\n",
            "Layer conv2_block3_3_bn: Trainable = False\n",
            "Layer conv2_block3_add: Trainable = False\n",
            "Layer conv2_block3_out: Trainable = False\n",
            "Layer conv3_block1_1_conv: Trainable = False\n",
            "Layer conv3_block1_1_bn: Trainable = False\n",
            "Layer conv3_block1_1_relu: Trainable = False\n",
            "Layer conv3_block1_2_conv: Trainable = False\n",
            "Layer conv3_block1_2_bn: Trainable = False\n",
            "Layer conv3_block1_2_relu: Trainable = False\n",
            "Layer conv3_block1_0_conv: Trainable = False\n",
            "Layer conv3_block1_3_conv: Trainable = False\n",
            "Layer conv3_block1_0_bn: Trainable = False\n",
            "Layer conv3_block1_3_bn: Trainable = False\n",
            "Layer conv3_block1_add: Trainable = False\n",
            "Layer conv3_block1_out: Trainable = False\n",
            "Layer conv3_block2_1_conv: Trainable = False\n",
            "Layer conv3_block2_1_bn: Trainable = False\n",
            "Layer conv3_block2_1_relu: Trainable = False\n",
            "Layer conv3_block2_2_conv: Trainable = False\n",
            "Layer conv3_block2_2_bn: Trainable = False\n",
            "Layer conv3_block2_2_relu: Trainable = False\n",
            "Layer conv3_block2_3_conv: Trainable = False\n",
            "Layer conv3_block2_3_bn: Trainable = False\n",
            "Layer conv3_block2_add: Trainable = False\n",
            "Layer conv3_block2_out: Trainable = False\n",
            "Layer conv3_block3_1_conv: Trainable = False\n",
            "Layer conv3_block3_1_bn: Trainable = False\n",
            "Layer conv3_block3_1_relu: Trainable = False\n",
            "Layer conv3_block3_2_conv: Trainable = False\n",
            "Layer conv3_block3_2_bn: Trainable = False\n",
            "Layer conv3_block3_2_relu: Trainable = False\n",
            "Layer conv3_block3_3_conv: Trainable = False\n",
            "Layer conv3_block3_3_bn: Trainable = False\n",
            "Layer conv3_block3_add: Trainable = False\n",
            "Layer conv3_block3_out: Trainable = False\n",
            "Layer conv3_block4_1_conv: Trainable = False\n",
            "Layer conv3_block4_1_bn: Trainable = False\n",
            "Layer conv3_block4_1_relu: Trainable = False\n",
            "Layer conv3_block4_2_conv: Trainable = False\n",
            "Layer conv3_block4_2_bn: Trainable = False\n",
            "Layer conv3_block4_2_relu: Trainable = False\n",
            "Layer conv3_block4_3_conv: Trainable = False\n",
            "Layer conv3_block4_3_bn: Trainable = False\n",
            "Layer conv3_block4_add: Trainable = False\n",
            "Layer conv3_block4_out: Trainable = False\n",
            "Layer conv4_block1_1_conv: Trainable = False\n",
            "Layer conv4_block1_1_bn: Trainable = False\n",
            "Layer conv4_block1_1_relu: Trainable = False\n",
            "Layer conv4_block1_2_conv: Trainable = False\n",
            "Layer conv4_block1_2_bn: Trainable = False\n",
            "Layer conv4_block1_2_relu: Trainable = False\n",
            "Layer conv4_block1_0_conv: Trainable = False\n",
            "Layer conv4_block1_3_conv: Trainable = False\n",
            "Layer conv4_block1_0_bn: Trainable = False\n",
            "Layer conv4_block1_3_bn: Trainable = False\n",
            "Layer conv4_block1_add: Trainable = False\n",
            "Layer conv4_block1_out: Trainable = False\n",
            "Layer conv4_block2_1_conv: Trainable = False\n",
            "Layer conv4_block2_1_bn: Trainable = False\n",
            "Layer conv4_block2_1_relu: Trainable = False\n",
            "Layer conv4_block2_2_conv: Trainable = False\n",
            "Layer conv4_block2_2_bn: Trainable = False\n",
            "Layer conv4_block2_2_relu: Trainable = False\n",
            "Layer conv4_block2_3_conv: Trainable = False\n",
            "Layer conv4_block2_3_bn: Trainable = False\n",
            "Layer conv4_block2_add: Trainable = False\n",
            "Layer conv4_block2_out: Trainable = False\n",
            "Layer conv4_block3_1_conv: Trainable = False\n",
            "Layer conv4_block3_1_bn: Trainable = False\n",
            "Layer conv4_block3_1_relu: Trainable = False\n",
            "Layer conv4_block3_2_conv: Trainable = False\n",
            "Layer conv4_block3_2_bn: Trainable = False\n",
            "Layer conv4_block3_2_relu: Trainable = False\n",
            "Layer conv4_block3_3_conv: Trainable = False\n",
            "Layer conv4_block3_3_bn: Trainable = False\n",
            "Layer conv4_block3_add: Trainable = False\n",
            "Layer conv4_block3_out: Trainable = False\n",
            "Layer conv4_block4_1_conv: Trainable = False\n",
            "Layer conv4_block4_1_bn: Trainable = False\n",
            "Layer conv4_block4_1_relu: Trainable = False\n",
            "Layer conv4_block4_2_conv: Trainable = False\n",
            "Layer conv4_block4_2_bn: Trainable = False\n",
            "Layer conv4_block4_2_relu: Trainable = False\n",
            "Layer conv4_block4_3_conv: Trainable = False\n",
            "Layer conv4_block4_3_bn: Trainable = False\n",
            "Layer conv4_block4_add: Trainable = False\n",
            "Layer conv4_block4_out: Trainable = False\n",
            "Layer conv4_block5_1_conv: Trainable = False\n",
            "Layer conv4_block5_1_bn: Trainable = False\n",
            "Layer conv4_block5_1_relu: Trainable = False\n",
            "Layer conv4_block5_2_conv: Trainable = False\n",
            "Layer conv4_block5_2_bn: Trainable = False\n",
            "Layer conv4_block5_2_relu: Trainable = False\n",
            "Layer conv4_block5_3_conv: Trainable = False\n",
            "Layer conv4_block5_3_bn: Trainable = False\n",
            "Layer conv4_block5_add: Trainable = False\n",
            "Layer conv4_block5_out: Trainable = False\n",
            "Layer conv4_block6_1_conv: Trainable = False\n",
            "Layer conv4_block6_1_bn: Trainable = False\n",
            "Layer conv4_block6_1_relu: Trainable = False\n",
            "Layer conv4_block6_2_conv: Trainable = False\n",
            "Layer conv4_block6_2_bn: Trainable = False\n",
            "Layer conv4_block6_2_relu: Trainable = False\n",
            "Layer conv4_block6_3_conv: Trainable = False\n",
            "Layer conv4_block6_3_bn: Trainable = False\n",
            "Layer conv4_block6_add: Trainable = False\n",
            "Layer conv4_block6_out: Trainable = False\n",
            "Layer conv5_block1_1_conv: Trainable = False\n",
            "Layer conv5_block1_1_bn: Trainable = False\n",
            "Layer conv5_block1_1_relu: Trainable = False\n",
            "Layer conv5_block1_2_conv: Trainable = False\n",
            "Layer conv5_block1_2_bn: Trainable = False\n",
            "Layer conv5_block1_2_relu: Trainable = False\n",
            "Layer conv5_block1_0_conv: Trainable = False\n",
            "Layer conv5_block1_3_conv: Trainable = False\n",
            "Layer conv5_block1_0_bn: Trainable = False\n",
            "Layer conv5_block1_3_bn: Trainable = False\n",
            "Layer conv5_block1_add: Trainable = False\n",
            "Layer conv5_block1_out: Trainable = False\n",
            "Layer conv5_block2_1_conv: Trainable = False\n",
            "Layer conv5_block2_1_bn: Trainable = False\n",
            "Layer conv5_block2_1_relu: Trainable = False\n",
            "Layer conv5_block2_2_conv: Trainable = False\n",
            "Layer conv5_block2_2_bn: Trainable = False\n",
            "Layer conv5_block2_2_relu: Trainable = False\n",
            "Layer conv5_block2_3_conv: Trainable = False\n",
            "Layer conv5_block2_3_bn: Trainable = False\n",
            "Layer conv5_block2_add: Trainable = False\n",
            "Layer conv5_block2_out: Trainable = False\n",
            "Layer conv5_block3_1_conv: Trainable = False\n",
            "Layer conv5_block3_1_bn: Trainable = False\n",
            "Layer conv5_block3_1_relu: Trainable = False\n",
            "Layer conv5_block3_2_conv: Trainable = False\n",
            "Layer conv5_block3_2_bn: Trainable = False\n",
            "Layer conv5_block3_2_relu: Trainable = False\n",
            "Layer conv5_block3_3_conv: Trainable = False\n",
            "Layer conv5_block3_3_bn: Trainable = False\n",
            "Layer conv5_block3_add: Trainable = False\n",
            "Layer conv5_block3_out: Trainable = False\n",
            "Layer avg_pool: Trainable = False\n"
          ]
        }
      ],
      "source": [
        "dnn_model = Sequential()\n",
        "\n",
        "imported_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3),\n",
        "    pooling='avg',\n",
        "    classes=10,\n",
        "    weights='imagenet')\n",
        "\n",
        "# Freeze all layers in the imported model\n",
        "for layer in imported_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Check if the layers are trainable\n",
        "for layer in imported_model.layers:\n",
        "    print(f\"Layer {layer.name}: Trainable = {layer.trainable}\")\n",
        "\n",
        "\n",
        "dnn_model.add(imported_model)\n",
        "dnn_model.add(Flatten())\n",
        "dnn_model.add(Dense(600, activation='relu'))\n",
        "dnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "dnn_model.compile(optimizer=Adam(lr=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training The Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 29s 1s/step - loss: 1.4727 - accuracy: 0.5562 - val_loss: 0.6949 - val_accuracy: 0.7656\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.3330 - accuracy: 0.8750 - val_loss: 0.4455 - val_accuracy: 0.8594\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 28s 1s/step - loss: 0.1927 - accuracy: 0.9344 - val_loss: 0.3571 - val_accuracy: 0.8875\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0981 - accuracy: 0.9750 - val_loss: 0.3730 - val_accuracy: 0.8750\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0767 - accuracy: 0.9812 - val_loss: 0.3497 - val_accuracy: 0.8891\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 31s 2s/step - loss: 0.0433 - accuracy: 0.9953 - val_loss: 0.4157 - val_accuracy: 0.8859\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 29s 1s/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.3983 - val_accuracy: 0.8750\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 28s 1s/step - loss: 0.0217 - accuracy: 0.9984 - val_loss: 0.3685 - val_accuracy: 0.8984\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 28s 1s/step - loss: 0.0258 - accuracy: 0.9969 - val_loss: 0.4134 - val_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 28s 1s/step - loss: 0.0296 - accuracy: 0.9937 - val_loss: 0.3479 - val_accuracy: 0.8984\n"
          ]
        }
      ],
      "source": [
        "history = dnn_model.fit(\n",
        "train_set,\n",
        "validation_data=valid_set,\n",
        "epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nj1i4Q4j73VZ"
      },
      "outputs": [],
      "source": [
        "dnn_model.save_weights('resnet_no_augmentation_batch20.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2942 - accuracy: 0.9000 - val_loss: 0.7538 - val_accuracy: 0.7688\n",
            "Epoch 2/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2508 - accuracy: 0.9125 - val_loss: 0.8231 - val_accuracy: 0.7500\n",
            "Epoch 3/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2690 - accuracy: 0.8938 - val_loss: 0.7567 - val_accuracy: 0.7563\n",
            "Epoch 4/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2930 - accuracy: 0.8875 - val_loss: 0.8769 - val_accuracy: 0.7688\n",
            "Epoch 5/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.4086 - accuracy: 0.8687 - val_loss: 0.8456 - val_accuracy: 0.7125\n",
            "Epoch 6/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.3574 - accuracy: 0.8938 - val_loss: 0.5972 - val_accuracy: 0.7812\n",
            "Epoch 7/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2987 - accuracy: 0.9062 - val_loss: 0.8477 - val_accuracy: 0.7812\n",
            "Epoch 8/15\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.2799 - accuracy: 0.9062 - val_loss: 0.7205 - val_accuracy: 0.7750\n",
            "Epoch 9/15\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.2697 - accuracy: 0.9062 - val_loss: 0.5848 - val_accuracy: 0.8125\n",
            "Epoch 10/15\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.2809 - accuracy: 0.9062 - val_loss: 0.9358 - val_accuracy: 0.7563\n",
            "Epoch 11/15\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3597 - accuracy: 0.8938 - val_loss: 1.0975 - val_accuracy: 0.6687\n",
            "Epoch 12/15\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3039 - accuracy: 0.8750 - val_loss: 0.9779 - val_accuracy: 0.7000\n",
            "Epoch 13/15\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.1809 - accuracy: 0.9500 - val_loss: 1.1150 - val_accuracy: 0.7250\n",
            "Epoch 14/15\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.2128 - accuracy: 0.9187 - val_loss: 0.7668 - val_accuracy: 0.7875\n",
            "Epoch 15/15\n",
            "5/5 [==============================] - 8s 2s/step - loss: 0.2246 - accuracy: 0.9062 - val_loss: 0.6662 - val_accuracy: 0.7937\n"
          ]
        }
      ],
      "source": [
        "history = dnn_model.fit(\n",
        "train_set,\n",
        "validation_data=valid_set,\n",
        "epochs=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
